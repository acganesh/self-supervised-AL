{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6248e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import sklearn\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from data.dataloaders import ImagesDataset\n",
    "from models.model import SelfSupervisedLearner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS     = 1000\n",
    "LR         = 3e-4\n",
    "IMAGE_SIZE = 96 # Change this depending on dataset\n",
    "NUM_GPUS= 0 # Change this depending on host\n",
    "NUM_WORKERS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1c492aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from  ./ckpt/learner_0510_v100.pt\n",
      "Train loading done\n",
      "Test loading done\n",
      "got embeddings\n"
     ]
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=False)\n",
    "model = SelfSupervisedLearner(\n",
    "    resnet,\n",
    "    image_size = IMAGE_SIZE,\n",
    "    hidden_layer = 'avgpool',\n",
    "    projection_size = 256,\n",
    "    projection_hidden_size = 4096,\n",
    "    moving_average_decay = 0.99,\n",
    "    lr = LR\n",
    ")\n",
    "\n",
    "    \n",
    "argv = [\"train.py\", \"--load\", \"./ckpt/learner_0510_v100.pt\"]\n",
    "model.load_state_dict(torch.load(argv[2]))\n",
    "print(\"Loaded checkpoint from \", argv[2])\n",
    "\n",
    "#TODO: for some reason labels don't exist in my wget data \n",
    "#ds = ImagesDataset(\"./dataset/test_images\", IMAGE_SIZE, train=False)\n",
    "data_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "train_dataset = torchvision.datasets.STL10('./dataset/train_split', split='train', download=False,\n",
    "                   transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=5000, num_workers=NUM_WORKERS, shuffle=False)\n",
    "\n",
    "\n",
    "train_imgs, train_labels = next(iter(train_loader))\n",
    "print(\"Train loading done\")\n",
    "\n",
    "test_dataset = torchvision.datasets.STL10('./dataset/test_split', split='test', download=False, transform=data_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8000, num_workers=NUM_WORKERS, shuffle=False)\n",
    "test_imgs, test_labels = next(iter(test_loader))\n",
    "print(\"Test loading done\")\n",
    "\n",
    "train_projs, train_embeddings = model.learner.forward(train_imgs, return_embedding=True)\n",
    "test_projs, test_embeddings = model.learner.forward(test_imgs, return_embedding=True)\n",
    "\n",
    "print(\"got embeddings\")\n",
    "\n",
    "train_imgs = torch.flatten(train_imgs, start_dim=1)\n",
    "test_imgs = torch.flatten(test_imgs, start_dim=1)\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(train_imgs)\n",
    "train_imgs = scaler.transform(train_imgs).astype(np.float32)\n",
    "test_imgs = scaler.transform(test_imgs).astype(np.float32)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=512)\n",
    "train_imgs_pca = pca.fit_transform(train_imgs)\n",
    "test_imgs_pca = pca.transform(test_imgs)\n",
    "\n",
    "lr_baseline = LogisticRegression(max_iter=100000)\n",
    "baseline_preds = lr_baseline.fit(train_imgs_pca, train_labels)\n",
    "\n",
    "baseline_preds = lr_baseline.predict_proba(test_imgs_pca)\n",
    "baseline_classes = lr_baseline.predict(test_imgs_pca)\n",
    "baseline_acc = sklearn.metrics.accuracy_score(test_labels, baseline_classes)\n",
    "\n",
    "lr_byol = LogisticRegression(max_iter=100000)\n",
    "lr_byol.fit(train_embeddings.detach().numpy(), train_labels)\n",
    "\n",
    "byol_preds = lr_byol.predict_proba(test_embeddings.detach().numpy())\n",
    "byol_classes = lr_byol.predict(test_embeddings.detach().numpy())\n",
    "byol_acc = sklearn.metrics.accuracy_score(test_labels, byol_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7fdb7d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr baseline:  0.154875\n",
      "random embeddings:  0.403125\n",
      "lr baseline:  0.17575\n",
      "random embeddings:  0.331625\n",
      "lr baseline:  0.19275\n",
      "random embeddings:  0.357125\n",
      "lr baseline:  0.1715\n",
      "random embeddings:  0.344875\n",
      "lr baseline:  0.1785\n",
      "random embeddings:  0.39175\n",
      "lr baseline:  0.212125\n",
      "random embeddings:  0.330875\n",
      "lr baseline:  0.162375\n",
      "random embeddings:  0.352\n",
      "lr baseline:  0.176375\n",
      "random embeddings:  0.349625\n",
      "lr baseline:  0.155125\n",
      "random embeddings:  0.3\n",
      "lr baseline:  0.170375\n",
      "random embeddings:  0.352125\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    random_idx = np.random.randint(0, high=train_imgs.shape[0], size = 25)\n",
    "\n",
    "    embeddings_subset = train_embeddings.detach().numpy()[random_idx]\n",
    "    train_labels_subset = train_labels[random_idx]\n",
    "\n",
    "    lr_rand = LogisticRegression(max_iter=100000)\n",
    "    lr_rand.fit(embeddings_subset, train_labels_subset)\n",
    "\n",
    "    rand_preds =lr_rand.predict(test_embeddings.detach().numpy())\n",
    "    rand_acc = sklearn.metrics.accuracy_score(test_labels, rand_preds)\n",
    "    \n",
    "    lr_baseline = LogisticRegression(max_iter=100000)\n",
    "    lr_baseline.fit(train_imgs[random_idx], train_labels_subset)\n",
    "    \n",
    "    lr_baseline_preds = lr_baseline.predict(test_imgs)\n",
    "    lr_baseline_acc = sklearn.metrics.accuracy_score(test_labels, lr_baseline_preds)\n",
    "    \n",
    "    print(\"lr baseline: \", lr_baseline_acc)\n",
    "\n",
    "    print(\"random embeddings: \", rand_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0d677cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=10, max_iter=100000)\n",
    "km.fit(train_embeddings.detach().numpy())\n",
    "\n",
    "clusters = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "74d0f800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9: 921, 5: 767, 7: 642, 4: 638, 2: 541, 0: 477, 3: 358, 1: 314, 8: 227, 6: 115})\n",
      "{4: 0.7836990595611286, 2: 0.9242144177449169, 9: 0.5428881650380022, 5: 0.651890482398957, 0: 1.0482180293501049, 3: 1.3966480446927376, 6: 4.347826086956522, 1: 1.5923566878980895, 7: 0.7788161993769471, 8: 2.202643171806167}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(clusters)\n",
    "total = train_embeddings.detach().numpy().shape[0]\n",
    "\n",
    "weights = {}\n",
    "uniform_prob = 0.1\n",
    "for k in counts:\n",
    "    weights[k] = uniform_prob / (counts[k] / total)\n",
    "    \n",
    "print(counts)\n",
    "print(weights)\n",
    "\n",
    "weights_full = [weights[k] for k in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "41f4073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "km:  0.37025\n",
      "lr baseline acc: 0.189625\n",
      "km:  0.26125\n",
      "lr baseline acc: 0.17625\n",
      "km:  0.344125\n",
      "lr baseline acc: 0.202625\n",
      "km:  0.353375\n",
      "lr baseline acc: 0.17825\n",
      "km:  0.396375\n",
      "lr baseline acc: 0.21675\n",
      "km:  0.3405\n",
      "lr baseline acc: 0.18375\n",
      "km:  0.4075\n",
      "lr baseline acc: 0.214\n",
      "km:  0.3765\n",
      "lr baseline acc: 0.166375\n",
      "km:  0.401875\n",
      "lr baseline acc: 0.18675\n",
      "km:  0.36025\n",
      "lr baseline acc: 0.211625\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for _ in range(10):\n",
    "    kmeans_idx = random.choices(range(train_imgs.shape[0]), weights=weights_full, k=25)\n",
    "\n",
    "    \"\"\"\n",
    "    cluster_subset = clusters[kmeans_idx]\n",
    "    cluster_counts = Counter(cluster_subset)\n",
    "    print(cluster_subset)\n",
    "    print(cluster_counts)\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings_subset = train_embeddings.detach().numpy()[kmeans_idx]\n",
    "    train_labels_subset = train_labels[kmeans_idx]\n",
    "\n",
    "    lr_km = LogisticRegression(max_iter=100000)\n",
    "    lr_km.fit(embeddings_subset, train_labels_subset)\n",
    "\n",
    "    km_preds =lr_km.predict(test_embeddings.detach().numpy())\n",
    "    km_acc = sklearn.metrics.accuracy_score(test_labels, km_preds)\n",
    "    \n",
    "    lr_baseline = LogisticRegression(max_iter=100000)\n",
    "    lr_baseline.fit(train_imgs[kmeans_idx], train_labels_subset)\n",
    "    \n",
    "    lr_baseline_preds = lr_baseline.predict(test_imgs)\n",
    "    lr_baseline_acc = sklearn.metrics.accuracy_score(test_labels, lr_baseline_preds)\n",
    "\n",
    "    print(\"km: \", km_acc)\n",
    "    print(\"lr baseline acc:\", lr_baseline_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce33ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
