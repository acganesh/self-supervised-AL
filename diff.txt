diff --git a/subset_selection.py b/subset_selection.py
index 3d770cd..1828799 100644
--- a/subset_selection.py
+++ b/subset_selection.py
@@ -1,3 +1,4 @@
+import datetime
 import multiprocessing
 import os
 import pickle
@@ -297,7 +298,7 @@ def get_predictions(data_dict, features_dict):
     return baseline_preds, baseline_acc, byol_preds, byol_acc
 
 
-def linear_eval(data_dict, features_dict, train_idx):
+def linear_eval(data_dict, features_dict, train_idx, metadata_dict):
     train_imgs = data_dict['train_imgs']
     train_labels = data_dict['train_labels']
     test_imgs = data_dict['test_imgs']
@@ -310,57 +311,90 @@ def linear_eval(data_dict, features_dict, train_idx):
     lr_baseline.fit(torch.flatten(train_imgs[train_idx], start_dim=1),
                     train_labels[train_idx])
 
+    lr_baseline_scores = lr_baseline.predict_proba(test_imgs)
     lr_baseline_preds = lr_baseline.predict(test_imgs)
-    lr_baseline_acc = sklearn.metrics.accuracy_score(test_labels,
-                                                     lr_baseline_preds)
 
     lr_byol = LogisticRegression(max_iter=100000)
     lr_byol.fit(train_embeddings[train_idx])
 
+    lr_byol_scores = lr_byol.predict_proba(test_imgs)
     lr_byol_preds = lr_byol.predict(test_imgs)
-    lr_byol_acc = sklearn.metrics.accuracy_score(test_labels, lr_byol_preds)
 
-    print("LR baseline acc: ", lr_baseline_acc)
-    print("LR BYOL acc: ", lr_byol_acc)
+    prediction_dict = {
+        'lr_baseline_scores': lr_baseline_scores,
+        'lr_baseline_preds': lr_baseline_preds,
+        'lr_byol_scores': lr_byol_scores,
+        'lr_byol_preds': lr_byol_preds
+    }
 
+    metrics_dict = compute_metrics(data_dict, prediction_dict)
+    log_metrics(metrics_dict, metadata_dict)
 
-def rand_sample(data_dict, features_dict):
-    train_imgs = data_dict['train_imgs']
-    train_labels = data_dict['train_labels']
+
+def get_timestamp():
+    return datetime.datetime.now().strftime('%Y%m%d%H%M%S')
+
+
+def compute_metrics(data_dict, prediction_dict):
+    lr_baseline_scores = prediction_dict['lr_baseline_scores']
+    lr_baseline_preds = prediction_dict['lr_baseline_preds']
     test_imgs = data_dict['test_imgs']
     test_labels = data_dict['test_labels']
-    train_embeddings = features_dict['train_embeddings']
-    test_embeddings = features_dict['test_embeddings']
 
-    random_idx = np.random.randint(0, high=train_imgs.shape[0], size=30)
+    lr_baseline_acc = sklearn.metrics.accuracy_score(test_labels,
+                                                     lr_baseline_preds)
+    lr_byol_acc = sklearn.metrics.accuracy_score(test_labels, lr_byol_preds)
 
-    embeddings_subset = train_embeddings.detach().cpu().numpy()[random_idx]
-    train_labels_subset = train_labels[random_idx]
+    # TODO: need to add to this.
+    metrics_dict = {
+        'lr_baseline_acc': lr_baseline_acc,
+        'lr_byol_acc': ly_byol_acc
+    }
 
-    lr_rand = LogisticRegression(max_iter=100000)
-    lr_rand.fit(embeddings_subset, train_labels_subset)
+    return metrics_dict
 
-    rand_preds = lr_rand.predict(test_embeddings.detach().cpu().numpy())
-    rand_acc = sklearn.metrics.accuracy_score(test_labels, rand_preds)
 
-    lr_baseline = LogisticRegression(max_iter=100000)
-    lr_baseline.fit(torch.flatten(train_imgs[random_idx], start_dim=1),
-                    train_labels_subset)
+def log_metrics(metrics_dict, metadata_dict):
+    ds_type = metadata_dict['ds_type']
+    sampler_type = metadata_dict['sampler_type']
+    num_examples = metadata_dict['num_examples']
 
-    lr_baseline_preds = lr_baseline.predict(
-        torch.flatten(test_imgs, start_dim=1))
-    lr_baseline_acc = sklearn.metrics.accuracy_score(test_labels,
-                                                     lr_baseline_preds)
+    timestamp = get_timestamp()
 
-    # rand_acc: accuracy of train lr on random byol embeddings
-    # lr_baseline_acc: accuracy of training lr on random images
-    print("lr baseline: ", lr_baseline_acc)
-    print("random embeddings: ", rand_acc)
+    metrics_path = f'./metrics/{timestamp}/'
+    if not os.path.exists(metrics_path):
+        os.makedirs(metrics_path)
 
-    return rand_acc, lr_baseline
+    fpath = os.path.join(
+        metrics_path, f'{ds_type}_{sampler_type}_{num_examples}examples.csv')
 
+    with open(fpath, 'rb') as f:
+        dict_writer = csv.DictWriter(f, metrics_dict.keys())
+        dict_writer.writeheader()
+        dict_writer.writerows([metrics_dict])
 
-def kmeans_sample(data_dict, features_dict):
+
+def rand_sample(data_dict, features_dict, num_examples, ds_type):
+    train_imgs = data_dict['train_imgs']
+    train_labels = data_dict['train_labels']
+    test_imgs = data_dict['test_imgs']
+    test_labels = data_dict['test_labels']
+    train_embeddings = features_dict['train_embeddings']
+    test_embeddings = features_dict['test_embeddings']
+
+    random_idx = np.random.randint(0,
+                                   high=train_imgs.shape[0],
+                                   size=num_examples)
+
+    metadata_dict = {
+        'sampler_type': 'rand',
+        'ds_type': ds_type,
+        'num_examples': num_examples
+    }
+    linear_eval(data_dict, features_dict, random_idx, metadata_dict)
+
+
+def kmeans_sample(data_dict, features_dict, num_examples, ds_type):
     train_imgs = data_dict['train_imgs']
     train_labels = data_dict['train_labels']
     test_imgs = data_dict['test_imgs']
@@ -385,7 +419,15 @@ def kmeans_sample(data_dict, features_dict):
 
     kmeans_idx = random.choices(range(train_imgs.shape[0]),
                                 weights=weights_full,
-                                k=30)
+                                k=num_examples)
+
+    metadata_dict = {
+        'sampler_type': 'kmeans',
+        'num_examples': num_examples,
+        'ds_type': ds_type
+    }
+    linear_eval(data_dict, features_dict, kmeans_idx, metadata_dict)
+    """
 
     embeddings_subset = train_embeddings.detach().cpu().numpy()[kmeans_idx]
     train_labels_subset = train_labels[kmeans_idx]
@@ -402,21 +444,19 @@ def kmeans_sample(data_dict, features_dict):
 
     lr_baseline_preds = lr_baseline.predict(
         torch.flatten(test_imgs, start_dim=1))
-    lr_baseline_acc = sklearn.metrics.accuracy_score(test_labels,
+    lr_baseline_acc = sklearn.metrics.accurajjjcy_score(test_labels,
                                                      lr_baseline_preds)
 
     print("km: ", km_acc)
     print("lr baseline acc:", lr_baseline_acc)
 
     return km_acc, lr_baseline_acc
+    """
 
 
 @torch.no_grad()
-def loss_based_ranking(model,
-                       data_dict,
-                       loader_dict,
-                       n_examples,
-                       num_forward_pass=5):
+def loss_based_ranking(model, data_dict, loader_dict, num_examples,
+                       num_forward_pass, ds_type):
     train_imgs = data_dict['train_imgs']
     train_labels = data_dict['train_labels']
 
@@ -445,14 +485,20 @@ def loss_based_ranking(model,
                         np.square(loss_means))
 
     idx = np.argsort(-loss_means)
-    mean_subset = idx[:n_examples]
+    mean_subset = idx[:num_examples]
     print("Mean Loss Eval:")
     linear_eval(data_dict, features_dict, mean_subset)
 
     idx = np.argsort(-loss_stds)
-    std_subset = idx[:n_examples]
+    std_subset = idx[:num_examples]
     print("STD Loss Eval:")
-    linear_eval(data_dict, features_dict, std_subset)
+
+    metadata_dict = {
+        'sampler_type': 'loss_based',
+        'num_examples': num_examples,
+        'ds_type': ds_type
+    }
+    linear_eval(data_dict, features_dict, std_subset, metadata_dict)
 
 
 def grad_based_ranking(model, data_dict, features_dict, loader_dict,
@@ -529,15 +575,21 @@ def main():
     print("Data and features loaded!")
 
     # TODO: AVERAGE ACROSS ACCS ACROSS N RUNS?
-    rand_sample(data_dict, features_dict)
-    kmeans_sample(data_dict, features_dict)
+
+    n_examples = 10
+    rand_sample(data_dict, features_dict, n_examples=n_examples)
+    kmeans_sample(data_dict, features_dict, n_examples=n_examples)
     train_imgs_subset, train_labels_subset = loss_based_ranking(
-        model, data_dict, loader_dict, n_examples=10, num_forward_pass=5)
+        model,
+        data_dict,
+        loader_dict,
+        n_examples=n_examples,
+        num_forward_pass=5)
     grad_based_ranking(model,
                        data_dict,
                        features_dict,
                        loader_dict,
-                       n_examples=10)
+                       n_examples=n_examples)
 
 
 if __name__ == '__main__':
